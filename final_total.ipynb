{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as Autograd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Set Class for single bidder 2 items\n",
    "'''\n",
    "class TypeData(Dataset):\n",
    "    def __init__(self, num_samples=10000, num_agents=1, num_items=2):\n",
    "        self.data = torch.rand(num_samples, num_agents * num_items)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Model & Valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Allocation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Allocation Model Class Definition\n",
    "'''\n",
    "class AdditiveAllocationRegretNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_agents, num_items, num_hidden_layers, num_neurons_per_hidden):\n",
    "        super(AdditiveAllocationRegretNet, self).__init__()\n",
    "        self.agents = num_agents # N agents,\n",
    "        self.items = num_items # L items, \n",
    "        self.types = self.agents * self.items # NxL vector of all types arranged (N,L)\n",
    "        self.hidlayers = num_hidden_layer # R = number of hidden layers\n",
    "        self.hidsize = neurons_per_hidden # K = number of neurons per hidden layer\n",
    "        \n",
    "        # input layer (LxN types) -> 1st hidden\n",
    "        self.layer1 = nn.Linear(in_features=self.types, out_features=self.hidsize)\n",
    "        \n",
    "        # adding R * Linear(K,K) hidden layers => adding R hidden layers with K neurons \n",
    "        self.layers = []\n",
    "        for _ in range(self.hidlayers):\n",
    "            temp_layer = nn.Linear(in_features=self.hidsize, out_features=self.num_hidden_size)\n",
    "            self.layers.append(temp_layer)\n",
    "        \n",
    "        # last hidden -> output layer (same size as input layer = LxN types)\n",
    "        self.final_layer = nn.Linear(in_features=self.hidsize, out_features=self.types)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # activated output of first layer\n",
    "        out = F.tanh(self.layer1(x))\n",
    "        # traverse all hidden layers\n",
    "        for i in range(self.hidlayers):\n",
    "            temp_out = self.layers[i](out)\n",
    "            out = F.tanh(temp_out)\n",
    "        # activated Final layer output\n",
    "        # applying softmax\n",
    "        out = self.final_layer(out)\n",
    "        out = out.view(self.agents, self.items)\n",
    "        out = torch.transpose(F.softmax(torch.transpose(x, 0, 1)), 0, 1)\n",
    "        out = out.view(self.agents * self.items)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Payments Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Payments Model Class Definition\n",
    "'''\n",
    "class AdditivePaymentsRegretNet(nn.Module):\n",
    "    \n",
    "    def _init__(self, num_agents, num_items, num_hidden_layers, num_neurons_per_hidden):\n",
    "        super(AdditivePaymentsRegretNet, self).__init__()\n",
    "        self.agents = num_agents # N agents,\n",
    "        self.items = num_items # L items, \n",
    "        self.types = num_types # NxL vector of all types arranged (N,L)\n",
    "        self.hidlayers = num_hidden_layer # R = number of hidden layers\n",
    "#         self.hidsize = neurons_per_hidden # K = number of neurons per hidden layer\n",
    "        \n",
    "        # input layer (NxL) -> 1st hidden layer (N)\n",
    "        self.layer1 = nn.Linear(in_features=self.types, out_features=self.agents)\n",
    "        \n",
    "        # adding R * Linear(N, N)\n",
    "        self.layers = []\n",
    "        for _ in range(self.hidlayers):\n",
    "            temp_layer = nn.Linear(in_features=self.agents, out_features=self.agents)\n",
    "            self.layers.append(temp_layer)\n",
    "        \n",
    "        # last hidden layer (N) -> output layer (N)\n",
    "        self.layerfinal = nn.Linear(in_features=self.agents, out_features=self.agents)\n",
    "    \n",
    "    def forward(self, bids, allocs):\n",
    "        # Processing through neural network\n",
    "        # activated output of first layer\n",
    "        out = F.tanh(self.layer1(bids))\n",
    "        # traverse all hidden layers\n",
    "        for i in range(self.hidlayers):\n",
    "            temp_out = self.layers[i](out)\n",
    "            out = F.tanh(temp_out)\n",
    "        # activated Final layer output\n",
    "        out_payment = F.sigmoid(self.layerfinal(out))\n",
    "        \n",
    "        # Generating final payments via allocation probs\n",
    "        bid_vals = allocs * bids\n",
    "        allocation_based_payments = torch.sum(bid_vals.view(self.agents, self.items), dim=1)\n",
    "        final_payments = out_payment * allocation_based_payments\n",
    "        return final_payments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp version for additive utilities only\n",
    "class UtilityFunction(Autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, allocation_model, payments_model, bids, types):\n",
    "        # Calculating Utilities\n",
    "        # generating allocation probabilities\n",
    "        allocs = allocation_model(bids)\n",
    "        # calculating adjusted valuation per prob of trade\n",
    "        vals_all = allocs * types\n",
    "        # generating agg valuation per agent\n",
    "        vals_reshaped = vals_all.view(allocation_model.agents, allocation_model.items)\n",
    "        vals = torch.sum(vals_reshaped, dim=1)\n",
    "        # determining payments\n",
    "        payments = payments_model(bids, allocs)\n",
    "        # caluclating utilities = valuations - payments\n",
    "        utilities = vals - payments\n",
    "        \n",
    "        # Saving Key Variables & Gradients\n",
    "        # acquiring reshaped version of types for easier backward functionality\n",
    "        reform_types = types.view(allocation_model.agents, allocation_model.items)\n",
    "        # acquiring dpdg for dudg (will save types directly)\n",
    "        d_pays_d_alloc = Autograd.grad(payments, allocs)\n",
    "        # acquiring d allocation d bid for d utility d bid\n",
    "        d_alloc_d_bid = Autograd.grad(allocs, bids).view(allocation_model.agents, allocation_model.items)\n",
    "        # acquiring d payment d bid for d utility d bid\n",
    "        d_pay_d_bid = Autograd.grad(payments, bids)\n",
    "        # saving all gradients and types\n",
    "        ctx.save_for_backward(reform_types, d_pay_d_alloc, d_alloc_d_bid, d_pay_d_bid)\n",
    "        \n",
    "        # returning for forward()\n",
    "        return utilities\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        '''\n",
    "        NOTE: do we need notion of upstream gradient fosho?\n",
    "        '''\n",
    "        types, dpaydalloc, dallocdbid, dpaydbid = ctx.saved_tensors\n",
    "        dudg = grad_output * (torch.sum(types, dim=1) - dpaydalloc)\n",
    "        dudp = -grad_output\n",
    "        dudb = grad_output * (torch.sum(dallocdbid * types, dim=1) - dpaydbid)\n",
    "        return dudg, dudp, dudb, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegretFunction(Autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, util, alloc_model, pay_model, bids, types):\n",
    "        # Calculating Regret\n",
    "        # strategic type utility calculation\n",
    "        util_bids = util(alloc_model, pay_model, bids, types)\n",
    "        # true type utility calculation\n",
    "        util_types = util(alloc_model, pay_model, types, types)\n",
    "        # regret = strategic util - true type util\n",
    "        regret_total = (util_bids - util_types)/alloc_model.items\n",
    "        \n",
    "        # Saving Gradients\n",
    "        # NOT SURE IF THIS WORKS MIGHT NEED TO REFORMAT\n",
    "        # saving grad wrt strategic utility\n",
    "        d_reg_d_bid = Audotgrad.grad(regret_total, util_bids)\n",
    "        # saving grad wrt true type util\n",
    "        d_reg_d_tru = Autograd.grad(regret_total, util_types)\n",
    "        # saving grad wrt g(*)\n",
    "        d_reg_d_alloc = Autograd.grad(regret_total, alloc_model(bids))\n",
    "        # saving grad wrt p(*)\n",
    "        d_reg_d_paym = Autograd.grad(regret_total, pay_model(bids))\n",
    "        # saving values\n",
    "        ctx.save_for_backward(d_reg_d_bid, d_reg_d_tru, d_reg_d_alloc, d_reg_d_paym, alloc_model.items)\n",
    "        \n",
    "        return regret_total\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        dregdbid, dregdtru, dregdallc, dregdpay, items = ctx.saved_tensors\n",
    "        drdb = grad_output * (dregdbid/items)\n",
    "        drdt = grad_output * (-dregdtru/items)\n",
    "        drdg = grad_output * (dregdallc)\n",
    "        drdp = grad_output * (dregdpay)\n",
    "        return None, drdg, drdp, drdb, drdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer and inner epochs\n",
    "epochs = 80\n",
    "misreport_epochs = 25\n",
    "\n",
    "# learning rates\n",
    "val_lr = 0.1\n",
    "pay_lr = 0.001\n",
    "all_lr = 0.001\n",
    "\n",
    "# batch sizes\n",
    "batch_size = 100\n",
    "# penalty terms\n",
    "rho = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our data sets from earlier\n",
    "trainset = TypeData(num_agents=1, num_items=2)\n",
    "valset = TypeData(num_agents=1, num_items=2)\n",
    "\n",
    "# Data Loaders are used to produce batches for training\n",
    "traindl = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valdl = DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Unsure how to execute RegretLoss without passing in payments model and generating re-inference\n",
    "'''\n",
    "class RegretLoss(Autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, payments, pay_grads, lambdas, regrets, regrets_grads, items, rho):\n",
    "        term1 = -torch.sum(payments)/items\n",
    "        term2 = torch.dot(lambdas, regrets)\n",
    "        term3 = (rho/2) * torch.dot(regrets, regrets)\n",
    "        loss = term1 + term2 + term3\n",
    "        ctx.save_for_backward(pay_grads, regrets_grads, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
